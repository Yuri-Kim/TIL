# F1-score

20200103

---

F1스코어 같은 메트릭들은 Classification을 할 수 있는 ML 알고리즘을 작성한 뒤 해당 모델이 얼마나 잘 작동하는지 통계적으로 확인할 때 사용됩니다. 가장 간단한 방법으로는 정확도(Accuracy)를 사용할 수 있겠죠?

Accuracy = 맞게 예측된 데이터수 / 전체 데이터 수 

즉, 전체중에 맞춘 갯수를 의미하죠! 그럼 F1 스코어는 무엇일까요? 정밀도(Precision)와 재현율(Recall)의 조화평균입니다.

정밀도(Precision)와 재현율(Recall) 그리고 조화 평균을 알아야겠네요.

짧게 설명하자면, 

- 정밀도(Precision) : 모델이 정답이라고 예측한 것중에 정답의 비율
- 재현율(Recall) : 실제 정답인 데이터 중 모델이 정답이라고 인식한 것의 비율
- 조화 평균 : 각 요소의 역수를 산술평균해서 그 값을 다시 역수로 변환한것!

F1 score를 사용하는 이유는 데이터 label이 불균형 구조일 때, 모델의 성능을 정확하게 평가할 수 있으며, 성능을 수치화 할 수 있기 때문입니다. accuracy는 label이 적은 것에 대해서는 예측 성능이 낮을 수 있기 때문이죠!

이제 위 내용들을 날씨 데이터에서 비가오는 날을 예측하는 경우에 빗대어서 설명해 볼게요!

정밀도(Precision)는, 모델이 비온 날이라고 예측한 것 중 정말 비가온날의 비율 / 재현율(Recall)은 이전에 비가 왔던 날들 중 모델이 비가 온날이라고 판단한 것의 비율이겠죠. 그런데 둘 중 한가지만 사용한다면 문제가 생깁니다. 예를 들어 한달 30일 동안 맑은 날이 25일이었는데, 확실한 7일만 맑다고 예측한다면, 당연히 맑다고 한 날 중에 실제 맑은 날(Precision)은 100%가 나오게 됩니다. 하지만 과연, 이러한 모델이 이상적인 모델일까요? 

따라서, 우리는 실제 맑은 25일 중에서 예측한 맑은 날의 수도 고려해 보아야합니다. 이 경우에는 정밀도(Precision)만큼 높은 결과가 나오지 않습니다. Precision과 함께 Recall을 함께 고려하면 실제 맑은 날들(즉, 분류의 대상이 되는 정의역, 실제 data)의 입장에서 우리의 모델이 맑다고 예측한 비율을 함께 고려하게 되어 제대로 평가할 수 있습니다. Precision과 Recall은 상호보완적으로 사용할 수 있으며, 두 지표가 모두 높을 수록 좋은 모델입니다.

그럼 왜 Accuracy가 아닌 F1 스코어 일까요 위의 예시를 보면, 모델이 30개 모두 맑은날이라고 예측해도 25/30의 높은 Accuracy를 갖게 됩니다. 즉 위에서 말한 것 처럼, label이 불균형 구조일 때, 모델의 성능을 정확하게 평가할 수 없겠죠!